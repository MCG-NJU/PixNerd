# lightning.pytorch==2.4.0
seed_everything: true
tags:
  exp: &exp pretraining_pix512
torch_hub_dir: /mnt/bn/wangshuai6/torch_hub
huggingface_cache_dir: null
trainer:
  default_root_dir: /mnt/bn/wangshuai6/universal_pix_t2i_pixnerd_workdirs
  accelerator: auto
  strategy: ddp
  devices: auto
  num_nodes: 1
  precision: bf16-true
  logger:
      class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        project: universal_pix_t2i_pixnerd_workdirs
        name: *exp
  num_sanity_val_steps: 2
  max_steps: 400000
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  val_check_interval: 10000
  check_val_every_n_epoch: null
  log_every_n_steps: 50
  deterministic: null
  inference_mode: true
  use_distributed_sampler: false
  callbacks:
    - class_path: src.callbacks.model_checkpoint.CheckpointHook
      init_args:
        every_n_train_steps: 20000
        save_top_k: -1
        save_last: true
    - class_path: src.callbacks.save_images.SaveImagesHook
      init_args:
         save_dir: val
  plugins:
    - src.plugins.bd_env.BDEnvironment
model:
  vae:
    class_path: src.models.autoencoder.pixel.PixelAE
  denoiser:
    class_path: src.models.transformer.pixnerd.PixNerDiT
    init_args:
      in_channels: 3
      patch_size: 16
      num_groups: 24
      hidden_size: &hidden_dim 1536
      txt_embed_dim: &txt_embed_dim 2048
      txt_max_length: 128
      num_text_blocks: 4
      decoder_hidden_size: 64
      num_encoder_blocks: 16
      num_decoder_blocks: 2
  conditioner:
    class_path: src.models.conditioner.qwen3_text_encoder.Qwen3TextEncoder
    init_args:
      weight_path: Qwen/Qwen3-1.7B
      embed_dim: *txt_embed_dim
      max_length: 128
  diffusion_trainer:
    class_path: src.diffusion.flow_matching.training_repa.REPATrainer
    init_args:
      lognorm_t: true
      feat_loss_weight: 0.5
      timeshift: 4.0
      encoder:
        class_path: src.models.encoder.DINOv2
        init_args:
            weight_path: /mnt/bn/wangshuai6/torch_hub/facebookresearch_dinov2_main/dinov2_vitb14
      align_layer: 6
      proj_denoiser_dim: *hidden_dim
      proj_hidden_dim: *hidden_dim
      proj_encoder_dim: 768
      scheduler: &scheduler src.diffusion.flow_matching.scheduling.LinearScheduler
  diffusion_sampler:
    class_path: src.diffusion.flow_matching.sampling.EulerSampler
    init_args:
      num_steps: 100
      guidance: 4.0
      scheduler: *scheduler
      w_scheduler: src.diffusion.flow_matching.scheduling.LinearScheduler
      guidance_fn: src.diffusion.base.guidance.simple_guidance_fn
      step_fn: src.diffusion.flow_matching.sampling.ode_step_fn
  ema_tracker:
    class_path: src.callbacks.simple_ema.SimpleEMA
    init_args:
      decay: 0.9999
  optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      betas:
        - 0.9
        - 0.95
      weight_decay: 0.00
data:
  eval_dataset:
    class_path: src.data.dataset.geneval.GenEvalDataset
    init_args:
      meta_json_path: ./evaluations/geneval/evaluation_metadata.jsonl
      num_samples_per_instance: 4
      latent_shape:
        - 3
        - 512
        - 512
  pred_dataset:
    class_path: src.data.dataset.geneval.GenEvalDataset
    init_args:
      meta_json_path: ./evaluations/geneval/evaluation_metadata_rephrased.jsonl
      num_samples_per_instance: 4
      latent_shape:
        - 3
        - 512
        - 512
  train_batch_size: 24
  train_num_workers: 4
  pred_batch_size: 8
  pred_num_workers: 1